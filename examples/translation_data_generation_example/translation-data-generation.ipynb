{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Data Generation Tutorial using Translation Pipeline\n",
    "\n",
    "This tutorial demonstrates how to use SDG repository to generate synthetic question-answer pairs from documents using large language models like ibm-granite/granite-3.3-2b-instruct. We'll cover:\n",
    "\n",
    "1. Setting up the environment\n",
    "2. Connecting to LLM servers\n",
    "3. Configuring the data generation pipeline\n",
    "4. Generating data with different models\n",
    "5. Comparing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable auto-reloading of modules - useful during development\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Instructions\n",
    "\n",
    "Before running this notebook, you'll need to:\n",
    "\n",
    "```bash \n",
    "pip install sdg-hub==0.1.0a2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "# datasets: For handling our data\n",
    "# OpenAI: For interfacing with the LLM servers\n",
    "# SDG components: For building our data generation pipeline\n",
    "from datasets import load_dataset, Dataset\n",
    "from openai import OpenAI\n",
    "\n",
    "from sdg_hub.flow import Flow\n",
    "from sdg_hub.pipeline import Pipeline\n",
    "from sdg_hub.sdg import SDG\n",
    "from sdg_hub.registry import PromptRegistry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up ibm-granite/granite-3.3-2b-instruct Model\n",
    "\n",
    "First, we need to host the Granite 3.3 model using vLLM. This creates an OpenAI-compatible API endpoint.\n",
    "\n",
    "1. Start the vLLM server (run in terminal):\n",
    "```bash\n",
    "CUDA_VISIBLE_DEVICES=0 python -m vllm.entrypoints.openai.api_server \\\n",
    "    --model ibm-granite/granite-3.3-2b-instruct \\\n",
    "    --dtype float16 \\\n",
    "    --tensor-parallel-size 1 \n",
    "```\n",
    "\n",
    "2. Connect to the model using OpenAI client below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure OpenAI client to connect to our local vLLM server\n",
    "endpoint = f\"http://localhost:8000/v1\"\n",
    "openai_api_key = \"EMPTY\"  # vLLM doesn't require real API key\n",
    "openai_api_base = endpoint\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "\n",
    "# Verify we can see the model\n",
    "teacher_model = client.models.list().data[0].id\n",
    "print(f\"Connected to model: {teacher_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure ibm-granite/granite-3.3-2b-instruct Prompt Template\n",
    "\n",
    "We need to register the correct chat template for our model to ensure proper prompt formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the ibm-granite/granite-3.3-2b-instruct chat template\n",
    "# This ensures proper formatting of prompts for the model\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the tokenizer to get the chat template\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ibm-granite/granite-3.3-2b-instruct\")\n",
    "\n",
    "# Register the chat template in our prompt registry\n",
    "@PromptRegistry.register(\"ibm-granite/granite-3.3-2b-instruct\")\n",
    "def granite_3_3_2b_chat_template():\n",
    "    return \"\"\"{# Alias tools -> available_tools #}\\n{%- if tools and not available_tools -%}\\n    {%- set available_tools = tools -%}\\n{%- endif -%}\\n{%- if messages[0]['role'] == 'system' %}\\n     {%- set system_message = messages[0]['content'] %}\\n     {%- set loop_messages = messages[1:] %}\\n {%- else %}\\n     {%- set system_message = \\\" Knowledge Cutoff Date: April 2024.\\n. You are Granite, developed by IBM.\\\" %}\\n     {%- if available_tools and documents %}\\n         {%- set system_message = system_message + \\\" You are a helpful assistant with access to the following tools. When a tool is required to answer the user's query, respond only with <|tool_call|> followed by a JSON list of tools used. If a tool does not exist in the provided list of tools, notify the user that you do not have the ability to fulfill the request. \\nWrite the response to the user's input by strictly aligning with the facts in the provided documents. If the information needed to answer the question is not available in the documents, inform the user that the question cannot be answered based on the available data.\\\" %}\\n     {%- elif available_tools %}\\n         {%- set system_message = system_message + \\\" You are a helpful assistant with access to the following tools. When a tool is required to answer the user's query, respond only with <|tool_call|> followed by a JSON list of tools used. If a tool does not exist in the provided list of tools, notify the user that you do not have the ability to fulfill the request.\\\" %}\\n     {%- elif documents %}\\n         {%- set system_message = system_message + \\\" Write the response to the user's input by strictly aligning with the facts in the provided documents. If the information needed to answer the question is not available in the documents, inform the user that the question cannot be answered based on the available data.\\\" %}\\n    {%- elif thinking %}\\n    {%- set system_message = system_message + \\\" You are a helpful AI assistant.\\nRespond to every user query in a comprehensive and detailed way. You can write down your thoughts and reasoning process before responding. In the thought process, engage in a comprehensive cycle of analysis, summarization, exploration, reassessment, reflection, backtracing, and iteration to develop well-considered thinking process. In the response section, based on various attempts, explorations, and reflections from the thoughts section, systematically present the final solution that you deem correct. The response should summarize the thought process. Write your thoughts between <think></think> and write your response between <response></response> for each user query.\\\" %}\\n     {%- else %}\\n         {%- set system_message = system_message + \\\" You are a helpful AI assistant.\\\" %}\\n     {%- endif %}\\n     {%- if 'citations' in controls and documents %}\\n         {%- set system_message = system_message + ' \\nUse the symbols <|start_of_cite|> and <|end_of_cite|> to indicate when a fact comes from a document in the search result, e.g <|start_of_cite|> {document_id: 1}my fact <|end_of_cite|> for a fact from document 1. Afterwards, list all the citations with their corresponding documents in an ordered list.' %}\\n     {%- endif %}\\n     {%- if 'hallucinations' in controls and documents %}\\n         {%- set system_message = system_message + ' \\nFinally, after the response is written, include a numbered list of sentences from the response with a corresponding risk value that are hallucinated and not based in the documents.' %}\\n     {%- endif %}\\n     {%- set loop_messages = messages %}\\n {%- endif %}\\n {{- '<|start_of_role|>system<|end_of_role|>' + system_message + '<|end_of_text|>\\n' }}\\n {%- if available_tools %}\\n     {{- '<|start_of_role|>available_tools<|end_of_role|>' }}\\n     {{- available_tools | tojson(indent=4) }}\\n     {{- '<|end_of_text|>\\n' }}\\n {%- endif %}\\n {%- if documents %}\\n     {%- for document in documents %}\\n         {{- '<|start_of_role|>document {\\\"document_id\\\": \\\"' + document['doc_id'] | string + '\\\"}<|end_of_role|>\\n' }}\\n         {{- document['text'] }}\\n         {{- '<|end_of_text|>\\n' }}\\n              {%- endfor %}\\n {%- endif %}\\n {%- for message in loop_messages %}\\n     {{- '<|start_of_role|>' + message['role'] + '<|end_of_role|>' + message['content'] + '<|end_of_text|>\\n' }}\\n     {%- if loop.last and add_generation_prompt %}\\n         {{- '<|start_of_role|>assistant' }}\\n             {%- if controls %}\\n                 {{- ' ' + controls | tojson()}}\\n             {%- endif %}\\n         {{- '<|end_of_role|>' }}\\n     {%- endif %}\\n {%- endfor %}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the Data Generation Pipeline\n",
    "\n",
    "Now we'll set up our Synthetic Data Generation (SDG) pipeline with the following components:\n",
    "1. SDG Flow configuration from YAML\n",
    "2. SDG Pipeline setup\n",
    "3. SDG configuration with batch processing, number of workers, and save frequency parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledge_agentic_pipeline = \"../../src/sdg_hub/flows/generation/knowledge/translate_flow_knowledge.yaml\"\n",
    "flow_cfg = Flow(client).get_flow_from_file(knowledge_agentic_pipeline)\n",
    "sdg = SDG(\n",
    "    [Pipeline(flow_cfg)],\n",
    "    num_workers=1,\n",
    "    batch_size=1,\n",
    "    save_freq=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Prepare Seed Data\n",
    "\n",
    "We'll load our seed data (documents) that will be used to generate question-answer pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the seed data from JSON file\n",
    "number_of_samples = 3\n",
    "seed_data_dir = f\"sdg_demo_output/\"\n",
    "ds = load_dataset('json', data_files=f'{seed_data_dir}/seed_data.jsonl', split='train')\n",
    "ds = ds.shuffle(seed=42).select(range(number_of_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Data with ibm-granite/granite-3.3-2b-instruct\n",
    "\n",
    "Now we'll use our configured pipeline to generate synthetic question-answer pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data and save checkpoints\n",
    "generated_data = sdg.generate(ds, checkpoint_dir=\"Tmp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Run SDG through python command (For large scale generation)\n",
    "\n",
    "```python\n",
    "python /home/lab/sdg/scripts/generate.py --ds_path {output_dir}/seed_data.jsonl --bs 8 --num_workers 8 --save_path {output_dir}/gen.jsonl --flow ../src/sdg_hub/flows/generation/knowledge/translate_knowledge.yaml --endpoint {teacher_endpoint_url} --checkpoint_dir {output_dir}/data_checkpoints --save_freq 2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save comparison results to markdown file\n",
    "output_file = f'{seed_data_dir}/generated_data.jsonl'\n",
    "\n",
    "generated_data.to_json(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
