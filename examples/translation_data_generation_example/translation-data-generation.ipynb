{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Data Generation Tutorial using Translation Pipeline\n",
    "\n",
    "This tutorial demonstrates how to use SDG repository to generate synthetic question-answer pairs from documents using large language models like LLaMA 3.3 70B. We will also generate data using Mixtral model for comparison. We'll cover:\n",
    "\n",
    "1. Setting up the environment\n",
    "2. Connecting to LLM servers\n",
    "3. Configuring the data generation pipeline\n",
    "4. Generating data with different models\n",
    "5. Comparing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable auto-reloading of modules - useful during development\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Instructions\n",
    "\n",
    "Before running this notebook, you'll need to:\n",
    "\n",
    "```bash \n",
    "pip install sdg-hub==0.1.0a2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rudramurthy/miniforge3/envs/py310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "# datasets: For handling our data\n",
    "# OpenAI: For interfacing with the LLM servers\n",
    "# SDG components: For building our data generation pipeline\n",
    "from datasets import load_dataset, Dataset\n",
    "from openai import OpenAI\n",
    "\n",
    "from sdg_hub.flow import Flow\n",
    "from sdg_hub.pipeline import Pipeline\n",
    "from sdg_hub.sdg import SDG\n",
    "from sdg_hub.registry import PromptRegistry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up LLaMA 3.3 70B Model\n",
    "\n",
    "First, we need to host the LLaMA model using vLLM. This creates an OpenAI-compatible API endpoint.\n",
    "\n",
    "1. Start the vLLM server (run in terminal):\n",
    "```bash\n",
    "CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 python -m vllm.entrypoints.openai.api_server \\\n",
    "    --model meta-llama/Llama-3.3-70B-Instruct \\\n",
    "    --dtype float16 \\\n",
    "    --tensor-parallel-size 8 \n",
    "```\n",
    "\n",
    "2. Connect to the model using OpenAI client below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[21:38:32] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> HTTP Request: <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">GET</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">http://localhost:8000/v1/models</span> <span style=\"color: #008000; text-decoration-color: #008000\">\"HTTP/1.1 200 OK\"</span>             <a href=\"file:///Users/rudramurthy/miniforge3/envs/py310/lib/python3.10/site-packages/httpx/_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/rudramurthy/miniforge3/envs/py310/lib/python3.10/site-packages/httpx/_client.py#1025\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1025</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[21:38:32]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m HTTP Request: \u001b[1;33mGET\u001b[0m \u001b[4;94mhttp://localhost:8000/v1/models\u001b[0m \u001b[32m\"HTTP/1.1 200 OK\"\u001b[0m             \u001b]8;id=979993;file:///Users/rudramurthy/miniforge3/envs/py310/lib/python3.10/site-packages/httpx/_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=328405;file:///Users/rudramurthy/miniforge3/envs/py310/lib/python3.10/site-packages/httpx/_client.py#1025\u001b\\\u001b[2m1025\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to model: Qwen/Qwen2.5-1.5B-Instruct\n"
     ]
    }
   ],
   "source": [
    "# Configure OpenAI client to connect to our local vLLM server\n",
    "endpoint = f\"http://localhost:8000/v1\"\n",
    "openai_api_key = \"EMPTY\"  # vLLM doesn't require real API key\n",
    "openai_api_base = endpoint\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "\n",
    "# Verify we can see the model\n",
    "teacher_model = client.models.list().data[0].id\n",
    "print(f\"Connected to model: {teacher_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Qwen 2.5 1B Prompt Template\n",
    "\n",
    "We need to register the correct chat template for our model to ensure proper prompt formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the LLaMA 3.3 chat template\n",
    "# This ensures proper formatting of prompts for the model\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the tokenizer to get the chat template\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-1.5B-Instruct\")\n",
    "\n",
    "# Register the chat template in our prompt registry\n",
    "@PromptRegistry.register(\"Qwen/Qwen2.5-1.5B-Instruct\")\n",
    "def llama_3_3_70b_chat_template():\n",
    "    return tokenizer.chat_template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the Data Generation Pipeline\n",
    "\n",
    "Now we'll set up our Synthetic Data Generation (SDG) pipeline with the following components:\n",
    "1. SDG Flow configuration from YAML\n",
    "2. SDG Pipeline setup\n",
    "3. SDG configuration with batch processing, number of workers, and save frequency parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledge_agentic_pipeline = \"/Users/rudramurthy/Documents/GitHub/new_ilab_sdg/src/sdg_hub/flows/generation/knowledge/translate_knowledge.yaml\"\n",
    "flow_cfg = Flow(client).get_flow_from_file(knowledge_agentic_pipeline)\n",
    "sdg = SDG(\n",
    "    [Pipeline(flow_cfg)],\n",
    "    num_workers=1,\n",
    "    batch_size=1,\n",
    "    save_freq=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Prepare Seed Data\n",
    "\n",
    "We'll load our seed data (documents) that will be used to generate question-answer pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the seed data from JSON file\n",
    "number_of_samples = 3\n",
    "seed_data_dir = f\"sdg_demo_output/\"\n",
    "ds = load_dataset('json', data_files=f'{seed_data_dir}/seed_data.jsonl', split='train')\n",
    "ds = ds.shuffle(seed=42).select(range(number_of_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Data with Qwen 2.5\n",
    "\n",
    "Now we'll use our configured pipeline to generate synthetic question-answer pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[21:44:20] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> No existing checkpoints found in Tmp, generating from scratch                        <a href=\"file:///Users/rudramurthy/Documents/GitHub/new_ilab_sdg/src/sdg_hub/sdg.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sdg.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/rudramurthy/Documents/GitHub/new_ilab_sdg/src/sdg_hub/sdg.py#107\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">107</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[21:44:20]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m No existing checkpoints found in Tmp, generating from scratch                        \u001b]8;id=605874;file:///Users/rudramurthy/Documents/GitHub/new_ilab_sdg/src/sdg_hub/sdg.py\u001b\\\u001b[2msdg.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=83270;file:///Users/rudramurthy/Documents/GitHub/new_ilab_sdg/src/sdg_hub/sdg.py#107\u001b\\\u001b[2m107\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Splitting the dataset into smaller batches                                           <a href=\"file:///Users/rudramurthy/Documents/GitHub/new_ilab_sdg/src/sdg_hub/sdg.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sdg.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/rudramurthy/Documents/GitHub/new_ilab_sdg/src/sdg_hub/sdg.py#123\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">123</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Splitting the dataset into smaller batches                                           \u001b]8;id=568676;file:///Users/rudramurthy/Documents/GitHub/new_ilab_sdg/src/sdg_hub/sdg.py\u001b\\\u001b[2msdg.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=462980;file:///Users/rudramurthy/Documents/GitHub/new_ilab_sdg/src/sdg_hub/sdg.py#123\u001b\\\u001b[2m123\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 38362.54it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Generating dataset with <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> splits, batch size <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, and <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> workers                        <a href=\"file:///Users/rudramurthy/Documents/GitHub/new_ilab_sdg/src/sdg_hub/sdg.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sdg.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/rudramurthy/Documents/GitHub/new_ilab_sdg/src/sdg_hub/sdg.py#129\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">129</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Generating dataset with \u001b[1;36m3\u001b[0m splits, batch size \u001b[1;36m1\u001b[0m, and \u001b[1;36m1\u001b[0m workers                        \u001b]8;id=2433;file:///Users/rudramurthy/Documents/GitHub/new_ilab_sdg/src/sdg_hub/sdg.py\u001b\\\u001b[2msdg.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=334455;file:///Users/rudramurthy/Documents/GitHub/new_ilab_sdg/src/sdg_hub/sdg.py#129\u001b\\\u001b[2m129\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Processing split <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>                                                                    <a href=\"file:///Users/rudramurthy/Documents/GitHub/new_ilab_sdg/src/sdg_hub/sdg.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sdg.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/rudramurthy/Documents/GitHub/new_ilab_sdg/src/sdg_hub/sdg.py#75\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">75</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Processing split \u001b[1;36m0\u001b[0m                                                                    \u001b]8;id=340068;file:///Users/rudramurthy/Documents/GitHub/new_ilab_sdg/src/sdg_hub/sdg.py\u001b\\\u001b[2msdg.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=429099;file:///Users/rudramurthy/Documents/GitHub/new_ilab_sdg/src/sdg_hub/sdg.py#75\u001b\\\u001b[2m75\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                  | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #808000; text-decoration-color: #808000\">WARNING </span> Loading facebook/nllb-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">200</span>-distilled-600M model<span style=\"color: #808000; text-decoration-color: #808000\">...</span>                        <a href=\"file:///Users/rudramurthy/Documents/GitHub/new_ilab_sdg/src/sdg_hub/blocks/translationblock.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">translationblock.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/rudramurthy/Documents/GitHub/new_ilab_sdg/src/sdg_hub/blocks/translationblock.py#76\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">76</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Loading facebook/nllb-\u001b[1;36m200\u001b[0m-distilled-600M model\u001b[33m...\u001b[0m                        \u001b]8;id=479921;file:///Users/rudramurthy/Documents/GitHub/new_ilab_sdg/src/sdg_hub/blocks/translationblock.py\u001b\\\u001b[2mtranslationblock.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=947548;file:///Users/rudramurthy/Documents/GitHub/new_ilab_sdg/src/sdg_hub/blocks/translationblock.py#76\u001b\\\u001b[2m76\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "document_translation Prompt Generation:   0%|                                                                                                                          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "document_translation Prompt Generation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:15<00:00, 15.56s/it]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[21:47:39] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> HTTP Request: <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">POST</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">http://localhost:8000/v1/completions</span> <span style=\"color: #008000; text-decoration-color: #008000\">\"HTTP/1.1 200 OK\"</span>       <a href=\"file:///Users/rudramurthy/miniforge3/envs/py310/lib/python3.10/site-packages/httpx/_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/rudramurthy/miniforge3/envs/py310/lib/python3.10/site-packages/httpx/_client.py#1025\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1025</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[21:47:39]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m HTTP Request: \u001b[1;33mPOST\u001b[0m \u001b[4;94mhttp://localhost:8000/v1/completions\u001b[0m \u001b[32m\"HTTP/1.1 200 OK\"\u001b[0m       \u001b]8;id=948744;file:///Users/rudramurthy/miniforge3/envs/py310/lib/python3.10/site-packages/httpx/_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=333580;file:///Users/rudramurthy/miniforge3/envs/py310/lib/python3.10/site-packages/httpx/_client.py#1025\u001b\\\u001b[2m1025\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">ERROR   </span> Error processing split <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>: Pipeline stopped: Empty dataset after running block:        <a href=\"file:///Users/rudramurthy/Documents/GitHub/new_ilab_sdg/src/sdg_hub/sdg.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sdg.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/rudramurthy/Documents/GitHub/new_ilab_sdg/src/sdg_hub/sdg.py#82\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">82</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>         question_response_generation                                                          <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[1;31mERROR   \u001b[0m Error processing split \u001b[1;36m0\u001b[0m: Pipeline stopped: Empty dataset after running block:        \u001b]8;id=995813;file:///Users/rudramurthy/Documents/GitHub/new_ilab_sdg/src/sdg_hub/sdg.py\u001b\\\u001b[2msdg.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=416319;file:///Users/rudramurthy/Documents/GitHub/new_ilab_sdg/src/sdg_hub/sdg.py#82\u001b\\\u001b[2m82\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m           \u001b[0m         question_response_generation                                                          \u001b[2m         \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/rudramurthy/Documents/GitHub/new_ilab_sdg/src/sdg_hub/sdg.py\", line 79, in _generate_data\n",
      "    input_split = pipeline.generate(input_split)\n",
      "  File \"/Users/rudramurthy/Documents/GitHub/new_ilab_sdg/src/sdg_hub/pipeline.py\", line 52, in generate\n",
      "    raise EmptyDatasetError(\n",
      "sdg_hub.pipeline.EmptyDatasetError: Pipeline stopped: Empty dataset after running block: question_response_generation\n",
      " 33%|███████████████████████████████████████████████████                                                                                                      | 1/3 [03:19<06:38, 199.28s/it]"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Processing split <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>                                                                    <a href=\"file:///Users/rudramurthy/Documents/GitHub/new_ilab_sdg/src/sdg_hub/sdg.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sdg.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/rudramurthy/Documents/GitHub/new_ilab_sdg/src/sdg_hub/sdg.py#75\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">75</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Processing split \u001b[1;36m1\u001b[0m                                                                    \u001b]8;id=905130;file:///Users/rudramurthy/Documents/GitHub/new_ilab_sdg/src/sdg_hub/sdg.py\u001b\\\u001b[2msdg.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=263254;file:///Users/rudramurthy/Documents/GitHub/new_ilab_sdg/src/sdg_hub/sdg.py#75\u001b\\\u001b[2m75\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #808000; text-decoration-color: #808000\">WARNING </span> Loading facebook/nllb-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">200</span>-distilled-600M model<span style=\"color: #808000; text-decoration-color: #808000\">...</span>                        <a href=\"file:///Users/rudramurthy/Documents/GitHub/new_ilab_sdg/src/sdg_hub/blocks/translationblock.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">translationblock.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/rudramurthy/Documents/GitHub/new_ilab_sdg/src/sdg_hub/blocks/translationblock.py#76\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">76</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Loading facebook/nllb-\u001b[1;36m200\u001b[0m-distilled-600M model\u001b[33m...\u001b[0m                        \u001b]8;id=676853;file:///Users/rudramurthy/Documents/GitHub/new_ilab_sdg/src/sdg_hub/blocks/translationblock.py\u001b\\\u001b[2mtranslationblock.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=350391;file:///Users/rudramurthy/Documents/GitHub/new_ilab_sdg/src/sdg_hub/blocks/translationblock.py#76\u001b\\\u001b[2m76\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "document_translation Prompt Generation:   0%|                                                                                                                          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "document_translation Prompt Generation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:15<00:00, 15.78s/it]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "# Generate synthetic data and save checkpoints\n",
    "generated_data = sdg.generate(ds, checkpoint_dir=\"Tmp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run SDG through python command (For large scale generation)\n",
    "\n",
    "```python\n",
    "python /home/lab/sdg/scripts/generate.py --ds_path {output_dir}/seed_data.jsonl --bs 8 --num_workers 8 --save_path {output_dir}/gen.jsonl --flow ../src/instructlab/sdg/flows/generation/knowledge/synth_knowledge1.5.yaml --endpoint {teacher_endpoint_url} --checkpoint_dir {output_dir}/data_checkpoints --save_freq 2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the generated data into training format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdg_hub.utils.parse_and_convert import create_knowledge_regular_ds, create_knowledge_pretraining_ds\n",
    "from datasets import concatenate_datasets\n",
    "\n",
    "output_dir = f\"sdg_demo_output/\"\n",
    "\n",
    "# Add the system prompt to final dataset if needed. For \n",
    "#  we use system prompt similar to below\n",
    "system_prompt_lab = (\n",
    "    \"I am a LAB Instruct Model, an AI language model developed by Red Hat and IBM Research based on the granite-3.1-8b-base model. My primary role is to serve as a chat assistant.\"\n",
    ")\n",
    "\n",
    "# This is a general instruction tuning dataset that is mixed with generated knowledge to train LLM simultaneously on your knowledge and general instructions.\n",
    "precomputed_skills_path = \"<LAB precomputed skills path>\"\n",
    "precomputed_skills = load_dataset('json', data_files=precomputed_skills_path, split='train')\n",
    "\n",
    "generated_ds = load_dataset('json', data_files=f'{output_dir}/gen.jsonl', split='train')\n",
    "\n",
    "# Create Pretraining Knowledge Dataset (Also known as Phase 0.7/Phase 7)\n",
    "phase_0_7_ds = create_knowledge_pretraining_ds(generated_ds)\n",
    "phase_0_7_ds.to_json(f'{output_dir}/phase_0_7_ds.jsonl', orient='records', lines=True)\n",
    "\n",
    "# Create Regular Knowledge Dataset (Also known as Phase 1.0/Phase 10)\n",
    "phase_1_ds = create_knowledge_regular_ds(generated_ds)\n",
    "\n",
    "# Mix the pre-computed skills with the regular knowledge dataset. If more than one dataset were generated simply add those in this concatenation stage.\n",
    "# If you have any generated instruction data, that can be also mixed in this stage. If you only have generated skills phase 07 generation and training can be skipped.\n",
    "phase_1_ds = concatenate_datasets([phase_1_ds, precomputed_skills])\n",
    "phase_1_ds.to_json(f'{output_dir}/phase_1_ds.jsonl', orient='records', lines=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
